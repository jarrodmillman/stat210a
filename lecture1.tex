\documentclass[twoside]{article}
\usepackage{amsmath,amssymb,amsthm,graphicx}
\usepackage{epsfig}
\usepackage[authoryear]{natbib}

\input{stat-macros}

\setlength{\oddsidemargin}{0.25 in}
\setlength{\evensidemargin}{-0.25 in}
\setlength{\topmargin}{-0.6 in}
\setlength{\textwidth}{6.5 in}
\setlength{\textheight}{8.5 in}
\setlength{\headsep}{0.75 in}
\setlength{\parindent}{0 in}
\setlength{\parskip}{0.1 in}

\newcommand{\lecture}[4]{
   \pagestyle{myheadings}
   \thispagestyle{plain}
   \newpage
   \setcounter{page}{1}
   \noindent
   \begin{center}
   \framebox{
      \vbox{\vspace{2mm}
    \hbox to 6.28in { {\bf Stat210A:~Theoretical Statistics \hfill Lecture Date: #4} }
       \vspace{6mm}
       \hbox to 6.28in { {\Large \hfill #1  \hfill}  }
       \vspace{6mm}
       \hbox to 6.28in { {\it Lecturer: #2 \hfill Scribe: #3} }
      \vspace{2mm}}
   }
   \end{center}
   \markboth{#1}{#1}
   \vspace*{4mm}
}

% Local Macros Put your favorite macros here that don't appear in
% stat-macros.tex.  We can eventually incorporate them into
% stat-macros.tex if they're of general use.

\begin{document}

\lecture{Lecture 1: Statistical Decision Theory}{Michael I. Jordan}{K. Jarrod
Millman}{August 28, 2014}

\section{Notation}

Statistical inference concerns learning from data.  In general, we will
consider data $X=(X_1,X_2,...,X_n)$ for $n \in \{1,2,...\}$ to be a random
vector (or variable).  In other words, we assume that there exists some
distribution $P_\theta$ such that $X \sim P_\theta$ where $P_\theta$  belongs
to a set $\cp = \{P_\theta : \theta \in \Omega\}$ of probability
distributions.  We call $\cp$ our statistical model for the distribtion
of $X$ and $\Omega$ our parameter space (i.e., the set of all possible values
of $\theta$).  For this semester, we will focus on parameteric models where
$\theta \subset \reals^P$.  Nonparameteric models will be covered in
210B.

A statistic $\delta(X)$ is a function of the data $X$ which may be real- or
funtion-valued.  In statistical estimation the goal is to find a statistic
$\delta$ such that $\delta(X)$ is "close to" $g(\theta)$, where $\theta$ is the
true parameter that indexes $X$'s distribution $P_\theta$.  We say that
$\delta$ is an estimator of $g(\theta)$.

\section{Decision Theory Framework}

To make precise what we mean by "close to" we introduce statistical decision
theory. Statistical decision theory was first developed by Abraham Wald and was
inspired by work in economics.

We first must introduce the notion of a loss function $L$, which given a parameter
$\theta$ and an estimate $\delta(x)$ returns a non-negative real number that
is the loss associated with estimating $g(\theta)$ with $\delta(x)$.  We also
require that there is no loss for estimating $g(\theta)$ with the correct
answer (i.e., $L(\theta, g(\theta))=0$).  For example, we may be interested
in the squared error as our loss function:
\begin{equation}
L(\theta, \delta(x)) = (\theta - \delta(x))^2.  \label{loss:squarederror}
\end{equation}

In practice, the true parameter $\theta$ is unknown and the statistic $\delta(X)$
is random. From the frequentist perspective, $\theta$ is assumed fixed (while
unknown) so the focus is on finding statistics that are "good" over lots of
different $x$.  For instance, if you are writing a general purpose software
package, then you want to provide some guarentee that your code will perform
well on lots of different data.  While the Bayesian will condition on the data
and treat the parameter $\theta$ as random.

\subsection{Frequentist risk}

The frequentist risk is defined as
\begin{equation}
R(\theta, \delta) = \ex_{\theta} L(\theta, \delta(X))
\end{equation}
where $\ex_{\theta}$ means that we are taking the expectation with respect to
$P_{\theta}$. Even though $X$ has been integrated out the frequentist risk
$R$ is still a function of $\theta$.

In certain situations, comparing the risk of two estimators is straightforward.
Consider the situation in Figure~\ref{fig:figure1}.  Here $R(\theta, \delta_2)$
always has lower risk than $R(\theta, \delta_1)$.  This motivates the following
definitions.

\begin{definition}\label{def:dominate}
  Given a specific loss function $R$ and two estimators $\delta_1$ and $\delta_2$,
  we say $\delta_2$ dominates $\delta_1$ if for all $\theta$ $R(\theta, \delta_2)
  \leq R(\theta, \delta_1)$ and there exists a $\theta$ such that $R(\theta,
  \delta_2) < R(\theta, \delta_1)$.
\end{definition}

\begin{definition}\label{def:inadmissable}
  Given a specific loss function $R$, an estimator $\delta_1$ is said to be
  inadmissable if there exists an estimator $\delta_2$ which dominates it. 
\end{definition}

\begin{figure}[ht]
\begin{minipage}[b]{0.45\linewidth}
\centering
\includegraphics[width=\textwidth]{fig/risk1-crop.pdf}
\caption{Two frequentist risk functions where one is dominated by the other.}
\label{fig:figure1}
\end{minipage}
\hspace{0.5cm}
\begin{minipage}[b]{0.45\linewidth}
\centering
\includegraphics[width=\textwidth]{fig/risk2-crop.pdf}
\caption{Two frequentist risk functions where neither is dominated by the other.}
\label{fig:figure2}
\end{minipage}
\end{figure}

To compare two admissible statistics (see Figure~\ref{fig:figure2}, we need
to impose additional constraints.

\begin{enumerate}
\item Initially a lot of attention was focused on unbiased estimators. An
  unbiased estimator is one where $\ex_{\theta}\delta(X) = g(\theta)$. It
  was often found that there exists a best (i.e., lowest variance) unbiased
  estimator.
\item \textbf{Equivariance}.
\item Minimax focuses on the worst case performance.  In other words, the estimator
   with lowest maximal risk is choosen.
\end{enumerate}

Constraints such as runtime are increasingly used in practice.

\subsection{Example}

For concreteness, consider \citep[Example 3.1, p.~40-41]{keener} the following
random variable
\begin{equation*}
X \sim Bin(100, \theta)
\end{equation*}
for $\theta \in [0,1]$.  We now examine the frequentist risk associated with
the following three estimators of $g(\theta) = \theta$ under the squared error
loss function:
\begin{equation*}
\delta_1(X) = \frac{X}{100}, \qquad \delta_2(X) = \frac{X+3}{100},
              \qquad \text{and} \quad \delta_3(X) = \frac{X+3}{106}.
\end{equation*}
We have the following risk functions for our three estimators:
\begin{align*}
R(\theta, \delta_1) &= \frac{\theta(1-\theta)}{100}\\
R(\theta, \delta_2) &= \frac{9+100 \theta(1-\theta)}{100^2}\\
R(\theta, \delta_3) &= \frac{(9-8\theta)(1+8\theta)}{106^2}
\end{align*}
for $\theta \in [0,1]$.  The three risk functions are plotted in
Figure~\ref{fig:figure3}. 

\begin{figure}[ht]
\centering
\includegraphics[width=0.4\textwidth]{fig/risk3-crop.pdf}
\caption{Frequentist risk functions for $\delta_1$, $\delta_2$, and $\delta_3$}
\label{fig:figure3}
\end{figure}

It should be clear from the figure that $\delta_2$ is inadmissable as both
of the other risk functions dominate it.  According to the minimax criteria,
you would choose $\delta_3$ over $\delta_1$ since it has better worse case
risk.  Using the traditional criteria of unbiasedness, you would choose
$\delta_1$ over $\delta_3$ since $\delta_1$ is unbiased while $\delta_3$ is
biased.  If we knew that $\theta$ should be close to $1/2$ then $\delta_3$
would give us lower frequentist risk, while if we knew that $\theta$ should
be closer to 0 or 1 then $\delta_1$ would give us lower risk.

\section{Bayes risk}

If we are only interested in particular values of $\theta$, it may be desirable
to weight those values more heavily.  This leads to Bayes' risk:
\begin{equation}
R(\delta) = \int R(\theta, \delta) \pi(\theta) d\theta.
\end{equation}
To make comparison of different statistics meaningful, $\pi(\theta)$ is
required to be a non-negative function of $\theta$ which integrates to 1.  For
frequentist, $\pi(\theta)$ is viewed as a weighting function.  For a Bayesian,
$\pi(\theta)$ is viewed as a prior on $\theta$. Regardless of how it is viewed,
if everyone agrees on a particular $\pi(\theta)$ then they can agree on
which estimator $\delta$ is best.

\bibliographystyle{apalike}
\bibliography{stat}

\end{document}
